{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zSJfJ8VgLqsf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Set Up Env.\n",
        "grid_rows = 2\n",
        "grid_cols = 3\n",
        "num_actions = 4\n",
        "\n",
        "# Define the reward matrix\n",
        "rewards = np.array([\n",
        "    [-10, 1, 0],\n",
        "    [0, -10, 10]\n",
        "]) # Here we have initialized the start state to be -10 as per the instructions given in the pdf\n",
        "\n",
        "# Define the Q-Table\n",
        "q_table = np.zeros((grid_rows, grid_cols, num_actions)) # Here we define a numpy 3d array that contains zeros throughout the array\n",
        "\n",
        "# Set hyper-parameters\n",
        "alpha = 0.1  # learning rate\n",
        "gamma = 0.9  # discount factor\n",
        "epsilon = 0.1  # exploration rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yMREUt1KOBe7"
      },
      "outputs": [],
      "source": [
        "# Defining a function to help us move in the grid\n",
        "def move(action, current_state):\n",
        "    next_state = current_state\n",
        "    if action == 0:  # In this step, we are moving upwards in the column\n",
        "        if current_state[0] > 0: # Here, we check to make sure we are not currently in the top row else we cannot move upwards\n",
        "            next_state = (current_state[0] - 1, current_state[1]) # If we get to this point, there is space to move upwards in the column so we update the index by reducing 1 to 'move upwards'\n",
        "    elif action == 1:  # In this step, we are moving downwards in the column\n",
        "        if current_state[0] < grid_rows - 1: # Here, we check to make sure we are not currently in the bottom row else we cannot move downwards\n",
        "            next_state = (current_state[0] + 1, current_state[1]) # If we get to this point, there is space to move downwards in the column so we update the index by adding 1 to 'move downwards'\n",
        "    elif action == 2:  # In this step, we are moving left in the row\n",
        "        if current_state[1] > 0: # Here, we check to make sure we are not currently in the left-most row else we cannot move left\n",
        "            next_state = (current_state[0], current_state[1] - 1) # If we get to this point, there is space to move left in the row so we update the index by reducing 1 to 'move left'\n",
        "    elif action == 3:  # In this step, we are moving right in the row\n",
        "        if current_state[1] < grid_cols - 1: # Here, we check to make sure we are not currently in the right-most row else we cannot move right\n",
        "            next_state = (current_state[0], current_state[1] + 1) # If we get to this point, there is space to move right in the row so we update the index by adding 1 to 'move right'\n",
        "\n",
        "    return next_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Fd3Tk2Dodxsz"
      },
      "outputs": [],
      "source": [
        "def q_value(current_state, next_state, action, reward):\n",
        "  if next_state is not None:\n",
        "    q_table[current_state][action] += alpha * (reward + gamma * np.max(q_table[next_state]) - q_table[current_state][action]) # This is the formula as shown in class to determine the q value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2VE_uRqdL0jl"
      },
      "outputs": [],
      "source": [
        "# Defining the function for the Q learning algorithm\n",
        "def q_learn(episodes):\n",
        "  for episode in range(episodes):\n",
        "    current_state = (0, 0)  # This is the start state and will also hold the current state.\n",
        "    completed = False # This will loop the program and will be updated when the goal is achieved or isn't reached in less than 4 moves.\n",
        "    number_of_moves = 0 # This keeps track of the number of moves made so far and is updated with every move.\n",
        "\n",
        "    while not completed:\n",
        "      randomNo = np.random.uniform() # This generates a random number between 0 and 1 to be used in exploration vs exploitation\n",
        "\n",
        "      # Exploration versus exploitation trade-off\n",
        "      if randomNo < epsilon:\n",
        "        action = np.random.randint(num_actions)  # To explore, a random action will be selected amongst up, down, left, and right\n",
        "      else:\n",
        "        action = np.argmax(q_table[current_state])  # To exploit, we select the action with the highest Q value to get the best chance at getting rewarded\n",
        "\n",
        "      next_state = current_state  # next_state is initialized to the current state. If there are no valid movements based on the action selected, the next state will remain the same as the current state.\n",
        "      reward = None # The reward for the current action has not yet been determined but will be updated based on the next state transition\n",
        "\n",
        "      next_state = move(action,current_state) # We call the `move` function and set its parameters\n",
        "\n",
        "      if next_state is not None: # This section will be run if a valid movement was made\n",
        "        reward = rewards[next_state] # We are rewarded based on the reward matrix defined for each state\n",
        "        if(reward == 10):\n",
        "          completed = True # We have reached the goal state\n",
        "      else:\n",
        "        reward = -1 # We haven't reached the goal state so we are given a -1 reward\n",
        "        number_of_moves += 1 # we have moved once so we add 1 to the number of moves made\n",
        "\n",
        "      q_value(current_state, next_state, action, reward) # We call the `q_value` function and set its parameters\n",
        "\n",
        "      current_state = next_state\n",
        "\n",
        "      if number_of_moves >= 4: # We stop the episode if the number of moves exceeds 3\n",
        "        completed = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fOGjZGtyWEfF"
      },
      "outputs": [],
      "source": [
        "q_learn(20_000) # We have set the number of episodes to 20000 as per instruction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTcwO_iZhPju",
        "outputId": "0158fc57-5e58-404c-f814-eb6cea681955"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[[-1.  0. -1. 10.]\n",
            "  [10. -1. -1.  9.]\n",
            "  [ 9. 10. 10.  9.]]\n",
            "\n",
            " [[-1.  0.  0. -1.]\n",
            "  [10. -1.  0. 10.]\n",
            "  [ 0.  0.  0.  0.]]]\n"
          ]
        }
      ],
      "source": [
        "print(q_table)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
